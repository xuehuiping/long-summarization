max_size of vocab was specified as 5000; we now have 5000 words. Stopping reading.
total bad lines: 0/0
Finished constructing vocabulary of 5000 total words. Last word added: parameterization
creating model...
using hierarchical attention mechanism for considering sections
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
Writing word embedding metadata file to logroot/arxiv_sample-experiment/train/vocab_metadata.tsv...
chkpt_dir for embeddings:  None
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
encoder_states.shape (4, ?, 512)
decoder_inputs[0].shape (4, 128)
Adding attention_decoder timesteps. 1 done of 21Adding attention_decoder timesteps. 2 done of 21Adding attention_decoder timesteps. 3 done of 21Adding attention_decoder timesteps. 4 done of 21Adding attention_decoder timesteps. 5 done of 21Adding attention_decoder timesteps. 6 done of 21Adding attention_decoder timesteps. 7 done of 21Adding attention_decoder timesteps. 8 done of 21Adding attention_decoder timesteps. 9 done of 21Adding attention_decoder timesteps. 10 done of 21Adding attention_decoder timesteps. 11 done of 21Adding attention_decoder timesteps. 12 done of 21Adding attention_decoder timesteps. 13 done of 21Adding attention_decoder timesteps. 14 done of 21Adding attention_decoder timesteps. 15 done of 21Adding attention_decoder timesteps. 16 done of 21Adding attention_decoder timesteps. 17 done of 21Adding attention_decoder timesteps. 18 done of 21Adding attention_decoder timesteps. 19 done of 21Adding attention_decoder timesteps. 20 done of 21Adding attention_decoder timesteps. 21 done of 21############################################################################## 
printing model variables:
seq2seq/embedding/embedding:0: shape=[5000, 128], variable_parameters=640000
seq2seq/encoder/word_level_encoder/bidirectional_rnn/fw/lstm_cell/kernel:0: shape=[384, 1024], variable_parameters=393216
seq2seq/encoder/word_level_encoder/bidirectional_rnn/fw/lstm_cell/bias:0: shape=[1024], variable_parameters=1024
seq2seq/encoder/word_level_encoder/bidirectional_rnn/bw/lstm_cell/kernel:0: shape=[384, 1024], variable_parameters=393216
seq2seq/encoder/word_level_encoder/bidirectional_rnn/bw/lstm_cell/bias:0: shape=[1024], variable_parameters=1024
seq2seq/encoder/word_level_encoder/reduce_final_st/w_reduce_c:0: shape=[512, 256], variable_parameters=131072
seq2seq/encoder/word_level_encoder/reduce_final_st/w_reduce_h:0: shape=[512, 256], variable_parameters=131072
seq2seq/encoder/word_level_encoder/reduce_final_st/bias_reduce_c:0: shape=[256], variable_parameters=256
seq2seq/encoder/word_level_encoder/reduce_final_st/bias_reduce_h:0: shape=[256], variable_parameters=256
seq2seq/encoder/section_level_encoder/bidirectional_rnn/fw/lstm_cell/kernel:0: shape=[512, 1024], variable_parameters=524288
seq2seq/encoder/section_level_encoder/bidirectional_rnn/fw/lstm_cell/bias:0: shape=[1024], variable_parameters=1024
seq2seq/encoder/section_level_encoder/bidirectional_rnn/bw/lstm_cell/kernel:0: shape=[512, 1024], variable_parameters=524288
seq2seq/encoder/section_level_encoder/bidirectional_rnn/bw/lstm_cell/bias:0: shape=[1024], variable_parameters=1024
seq2seq/encoder/section_level_encoder/reduce_final_st/w_reduce_c:0: shape=[512, 256], variable_parameters=131072
seq2seq/encoder/section_level_encoder/reduce_final_st/w_reduce_h:0: shape=[512, 256], variable_parameters=131072
seq2seq/encoder/section_level_encoder/reduce_final_st/bias_reduce_c:0: shape=[256], variable_parameters=256
seq2seq/encoder/section_level_encoder/reduce_final_st/bias_reduce_h:0: shape=[256], variable_parameters=256
seq2seq/decoder/attention_decoder/W_h:0: shape=[1, 1, 512, 512], variable_parameters=262144
seq2seq/decoder/attention_decoder/W_h_s:0: shape=[1, 1, 512, 512], variable_parameters=262144
seq2seq/decoder/attention_decoder/v_sec:0: shape=[512], variable_parameters=512
seq2seq/decoder/attention_decoder/v:0: shape=[512], variable_parameters=512
seq2seq/decoder/attention_decoder/Linear/Matrix:0: shape=[640, 128], variable_parameters=81920
seq2seq/decoder/attention_decoder/Linear/Bias:0: shape=[128], variable_parameters=128
seq2seq/decoder/attention_decoder/lstm_cell/kernel:0: shape=[384, 1024], variable_parameters=393216
seq2seq/decoder/attention_decoder/lstm_cell/bias:0: shape=[1024], variable_parameters=1024
seq2seq/decoder/attention_decoder/Attention/Linear/Matrix:0: shape=[512, 512], variable_parameters=262144
seq2seq/decoder/attention_decoder/Attention/Linear/Bias:0: shape=[512], variable_parameters=512
seq2seq/decoder/attention_decoder/Attention/attention_sections/Linear--Section-Features/Matrix:0: shape=[512, 512], variable_parameters=262144
seq2seq/decoder/attention_decoder/Attention/attention_sections/Linear--Section-Features/Bias:0: shape=[512], variable_parameters=512
seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Matrix:0: shape=[1152, 1], variable_parameters=1152
seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Bias:0: shape=[1], variable_parameters=1
seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Matrix:0: shape=[768, 256], variable_parameters=196608
seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Bias:0: shape=[256], variable_parameters=256
seq2seq/output_projection/w:0: shape=[256, 5000], variable_parameters=1280000
seq2seq/output_projection/b:0: shape=[5000], variable_parameters=5000
total model parameters: 6014345
##############################################################################
total in queue: 200 of 200
total in queue: 200 of 200
total in queue: 200 of 200
total in queue: 200 of 200
total in queue: 200 of 200
total in queue: 200 of 200
total in queue: 200 of 200
total in queue: 200 of 200
total in queue: 200 of 200
total in queue: 200 of 200
total in queue: 200 of 200
total in queue: 200 of 200
max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.
total bad lines: 0/0
Finished constructing vocabulary of 50000 total words. Last word added: -hard
creating model...
using hierarchical attention mechanism for considering sections
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
total in queue: 100 of 100
Writing word embedding metadata file to logroot/arxiv_sample-experiment/train/vocab_metadata.tsv...
chkpt_dir for embeddings:  logroot/arxiv_sample-experiment/train/model.ckpt-0
encoder_states.shape (4, ?, 512)
decoder_inputs[0].shape (4, 128)
Adding attention_decoder timesteps. 1 done of 21Adding attention_decoder timesteps. 2 done of 21Adding attention_decoder timesteps. 3 done of 21Adding attention_decoder timesteps. 4 done of 21Adding attention_decoder timesteps. 5 done of 21Adding attention_decoder timesteps. 6 done of 21Adding attention_decoder timesteps. 7 done of 21Adding attention_decoder timesteps. 8 done of 21Adding attention_decoder timesteps. 9 done of 21Adding attention_decoder timesteps. 10 done of 21Adding attention_decoder timesteps. 11 done of 21Adding attention_decoder timesteps. 12 done of 21Adding attention_decoder timesteps. 13 done of 21Adding attention_decoder timesteps. 14 done of 21Adding attention_decoder timesteps. 15 done of 21Adding attention_decoder timesteps. 16 done of 21Adding attention_decoder timesteps. 17 done of 21Adding attention_decoder timesteps. 18 done of 21Adding attention_decoder timesteps. 19 done of 21Adding attention_decoder timesteps. 20 done of 21Adding attention_decoder timesteps. 21 done of 21############################################################################## 
printing model variables:
seq2seq/embedding/embedding:0: shape=[50000, 128], variable_parameters=6400000
seq2seq/encoder/word_level_encoder/bidirectional_rnn/fw/lstm_cell/kernel:0: shape=[384, 1024], variable_parameters=393216
seq2seq/encoder/word_level_encoder/bidirectional_rnn/fw/lstm_cell/bias:0: shape=[1024], variable_parameters=1024
seq2seq/encoder/word_level_encoder/bidirectional_rnn/bw/lstm_cell/kernel:0: shape=[384, 1024], variable_parameters=393216
seq2seq/encoder/word_level_encoder/bidirectional_rnn/bw/lstm_cell/bias:0: shape=[1024], variable_parameters=1024
seq2seq/encoder/word_level_encoder/reduce_final_st/w_reduce_c:0: shape=[512, 256], variable_parameters=131072
seq2seq/encoder/word_level_encoder/reduce_final_st/w_reduce_h:0: shape=[512, 256], variable_parameters=131072
seq2seq/encoder/word_level_encoder/reduce_final_st/bias_reduce_c:0: shape=[256], variable_parameters=256
seq2seq/encoder/word_level_encoder/reduce_final_st/bias_reduce_h:0: shape=[256], variable_parameters=256
seq2seq/encoder/section_level_encoder/bidirectional_rnn/fw/lstm_cell/kernel:0: shape=[512, 1024], variable_parameters=524288
seq2seq/encoder/section_level_encoder/bidirectional_rnn/fw/lstm_cell/bias:0: shape=[1024], variable_parameters=1024
seq2seq/encoder/section_level_encoder/bidirectional_rnn/bw/lstm_cell/kernel:0: shape=[512, 1024], variable_parameters=524288
seq2seq/encoder/section_level_encoder/bidirectional_rnn/bw/lstm_cell/bias:0: shape=[1024], variable_parameters=1024
seq2seq/encoder/section_level_encoder/reduce_final_st/w_reduce_c:0: shape=[512, 256], variable_parameters=131072
seq2seq/encoder/section_level_encoder/reduce_final_st/w_reduce_h:0: shape=[512, 256], variable_parameters=131072
seq2seq/encoder/section_level_encoder/reduce_final_st/bias_reduce_c:0: shape=[256], variable_parameters=256
seq2seq/encoder/section_level_encoder/reduce_final_st/bias_reduce_h:0: shape=[256], variable_parameters=256
seq2seq/decoder/attention_decoder/W_h:0: shape=[1, 1, 512, 512], variable_parameters=262144
seq2seq/decoder/attention_decoder/W_h_s:0: shape=[1, 1, 512, 512], variable_parameters=262144
seq2seq/decoder/attention_decoder/v_sec:0: shape=[512], variable_parameters=512
seq2seq/decoder/attention_decoder/v:0: shape=[512], variable_parameters=512
seq2seq/decoder/attention_decoder/Linear/Matrix:0: shape=[640, 128], variable_parameters=81920
seq2seq/decoder/attention_decoder/Linear/Bias:0: shape=[128], variable_parameters=128
seq2seq/decoder/attention_decoder/lstm_cell/kernel:0: shape=[384, 1024], variable_parameters=393216
seq2seq/decoder/attention_decoder/lstm_cell/bias:0: shape=[1024], variable_parameters=1024
seq2seq/decoder/attention_decoder/Attention/Linear/Matrix:0: shape=[512, 512], variable_parameters=262144
seq2seq/decoder/attention_decoder/Attention/Linear/Bias:0: shape=[512], variable_parameters=512
seq2seq/decoder/attention_decoder/Attention/attention_sections/Linear--Section-Features/Matrix:0: shape=[512, 512], variable_parameters=262144
seq2seq/decoder/attention_decoder/Attention/attention_sections/Linear--Section-Features/Bias:0: shape=[512], variable_parameters=512
seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Matrix:0: shape=[1152, 1], variable_parameters=1152
seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Bias:0: shape=[1], variable_parameters=1
seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Matrix:0: shape=[768, 256], variable_parameters=196608
seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Bias:0: shape=[256], variable_parameters=256
seq2seq/output_projection/w:0: shape=[256, 50000], variable_parameters=12800000
seq2seq/output_projection/b:0: shape=[50000], variable_parameters=50000
total model parameters: 23339345
##############################################################################
